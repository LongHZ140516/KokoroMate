import yaml
import json
import base64
import uvicorn
import librosa
import model_function
import os
from fastapi import FastAPI, Request, File, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from fastapi import HTTPException

config = yaml.safe_load(open("./frontend/public/default.yaml", "r", encoding="utf-8"))

# set system prompt
system_prompt = model_function.build_system_prompt(config)

app = FastAPI()

# ç¡®ä¿cacheç›®å½•å­˜åœ¨
if not os.path.exists("cache"):
    os.makedirs("cache")
    print("âœ… Created cache directory")

# ç¡®ä¿èŠå¤©è®°å½•ç›®å½•å­˜åœ¨
if not os.path.exists("chat_history"):
    os.makedirs("chat_history")
    print("âœ… Created chat history directory")

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],  # å…è®¸å‰ç«¯åœ°å€
    allow_credentials=True,
    allow_methods=["*"],  # å…è®¸æ‰€æœ‰HTTPæ–¹æ³•
    allow_headers=["*"],  # å…è®¸æ‰€æœ‰è¯·æ±‚å¤´
)

# è‡ªå®šä¹‰éŸ³é¢‘æ–‡ä»¶æœåŠ¡ï¼Œæ·»åŠ å¿…è¦çš„å¤´éƒ¨
@app.get("/audio/{filename}")
async def serve_audio(filename: str):
    file_path = os.path.join("cache", filename)
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="Audio file not found")
    
    # è¿”å›æ–‡ä»¶å“åº”ï¼Œæ·»åŠ å¿…è¦çš„å¤´éƒ¨
    response = FileResponse(
        path=file_path,
        media_type="audio/wav",
        headers={
            "Cross-Origin-Resource-Policy": "cross-origin",
            "Cross-Origin-Embedder-Policy": "unsafe-none",
            "Access-Control-Allow-Origin": "*",
            "Cache-Control": "no-cache"
        }
    )
    return response

# åŸæ¥çš„é™æ€æ–‡ä»¶æœåŠ¡å·²è¢«ä¸Šé¢çš„è‡ªå®šä¹‰è·¯ç”±æ›¿ä»£
# app.mount("/audio", StaticFiles(directory="cache"), name="audio")

if config["system"]["chat_mode"] == "text_only":
    llm_model = model_function.set_llm_model(config["system"]["default_model"]["llm"], config["llm"][config["system"]["default_model"]["llm"]])
    tts_model = model_function.set_tts_model(config["system"]["default_model"]["tts"], config["tts"][config["system"]["default_model"]["tts"]])

else:
    asr_model = model_function.set_asr_model(config["system"]["default_model"]["asr"], config["asr"][config["system"]["default_model"]["asr"]])
    llm_model = model_function.set_llm_model(config["system"]["default_model"]["llm"], config["llm"][config["system"]["default_model"]["llm"]])
    tts_model = model_function.set_tts_model(config["system"]["default_model"]["tts"], config["tts"][config["system"]["default_model"]["tts"]])

@app.post("/chat_api/text")
async def chat_api_text(request: Request):
    chat_data = await request.json()
    input_text = chat_data.get("input_text")
    # input_file = chat_data.get("input_file")
    messages = [
        {
            "role": "system",
            "content": system_prompt
        },
        {
            "role": "user",
            "content": input_text
        }
    ]

    response_generator = llm_model.chat_completion(messages)
    full_response = ""

    async for chunk in response_generator:
        full_response += chunk

    response = model_function.extract_json_from_markdown(full_response)
    
    # æ£€æŸ¥JSONè§£ææ˜¯å¦æˆåŠŸ
    if response is None:
        print(f"âŒ Failed to parse JSON from LLM response: {full_response}")
        response = {"text": "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å¤„ç†æ‚¨çš„è¯·æ±‚ï¼Œè¯·ç¨åé‡è¯•ã€‚", "motion": "idle"}
    
    response_text, response_motion = response.get("text"), response.get("motion")
    tts_file_path = tts_model.generate_speech(response_text)
    
    print(f"ğŸ” TTS file path: {tts_file_path}")
    
    # å°†TTSè¿”å›çš„æœ¬åœ°è·¯å¾„è½¬æ¢ä¸ºå‰ç«¯å¯è®¿é—®çš„è·¯å¾„
    if tts_file_path and tts_file_path.startswith("cache/"):
        audio_path = f"/audio/{tts_file_path.replace('cache/', '')}"
        print(f"âœ… Converted audio path: {audio_path}")
    else:
        audio_path = tts_file_path
        print(f"âš ï¸ Using original path: {audio_path}")

    return {"text": response_text, "motion": response_motion, "audio_path": audio_path}

@app.post("/chat_api/audio")
async def chat_api_audio(audio_file: UploadFile = File(...)):
    print("ğŸ¤ Received audio file upload")
    
    if config["system"]["chat_mode"] == "text_only":
        print("âŒ Audio mode is disabled in config")
        return {"error": "Audio mode is not supported in text only mode"}
    
    try:
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        if not audio_file.content_type.startswith('audio/'):
            print(f"âŒ Invalid file type: {audio_file.content_type}")
            return {"error": "Invalid file type. Please upload an audio file."}
        
        print(f"ğŸ¤ Audio file received: {audio_file.filename}, size: {audio_file.size}, type: {audio_file.content_type}")
        
        # è¯»å–éŸ³é¢‘æ–‡ä»¶å†…å®¹
        audio_content = await audio_file.read()
        
        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        temp_audio_path = "temp_audio.wav"
        with open(temp_audio_path, "wb") as f:
            f.write(audio_content)
        
        print(f"ğŸ¤ Saved temp audio file: {temp_audio_path}")
        
        # ä½¿ç”¨librosaåŠ è½½éŸ³é¢‘æ–‡ä»¶
        try:
            audio_array, sample_rate = librosa.load(temp_audio_path, sr=16000)
            print(f"ğŸ¤ Audio loaded with librosa: shape={audio_array.shape}, sample_rate={sample_rate}")
        except Exception as e:
            print(f"âŒ Failed to load audio with librosa module, trying wave: {e}")
            audio_array, sample_rate = model_function.load_wav_file(temp_audio_path)
            print(f"ğŸ¤ Audio loaded with wave: shape={audio_array.shape}, sample_rate={sample_rate}")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_audio_path):
            os.remove(temp_audio_path)
        
        print("ğŸ¤ Starting ASR...")
        input_text = asr_model.audio2text(audio_array)
        print(f"ğŸ¤ ASR result: {input_text}")
        
    except Exception as e:
        print(f"âŒ Error processing audio: {e}")
        import traceback
        traceback.print_exc()
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists("temp_audio.wav"):
            os.remove("temp_audio.wav")
        return {"error": f"Audio processing failed: {str(e)}"}
    
    messages = [
        {
            "role": "system",
            "content": system_prompt
        },
        {
            "role": "user",
            "content": input_text
        }
    ]

    response_generator = llm_model.chat_completion(messages)
    full_response = ""

    async for chunk in response_generator:
        full_response += chunk

    response = model_function.extract_json_from_markdown(full_response)
    
    # æ£€æŸ¥JSONè§£ææ˜¯å¦æˆåŠŸ
    if response is None:
        print(f"âŒ Failed to parse JSON from LLM response: {full_response}")
        response = {"text": "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•ç†è§£æ‚¨çš„è¯­éŸ³è¾“å…¥ï¼Œè¯·ç¨åé‡è¯•ã€‚", "motion": "idle"}
    
    response_text, response_motion = response.get("text"), response.get("motion")
    tts_file_path = tts_model.generate_speech(response_text)
    
    print(f"ğŸ” Audio TTS file path: {tts_file_path}")
    
    # å°†TTSè¿”å›çš„æœ¬åœ°è·¯å¾„è½¬æ¢ä¸ºå‰ç«¯å¯è®¿é—®çš„è·¯å¾„
    if tts_file_path and tts_file_path.startswith("cache/"):
        audio_path = f"/audio/{tts_file_path.replace('cache/', '')}"
        print(f"âœ… Audio converted path: {audio_path}")
    else:
        audio_path = tts_file_path
        print(f"âš ï¸ Audio using original path: {audio_path}")

    return {"asr_text": input_text, "text": response_text, "motion": response_motion, "audio_path": audio_path}

# èŠå¤©è®°å½•ç®¡ç†API
@app.get("/chat_history")
async def get_chat_history():
    """è·å–èŠå¤©è®°å½•"""
    try:
        history_file = "chat_history/chat.json"
        if os.path.exists(history_file):
            with open(history_file, 'r', encoding='utf-8') as f:
                history = json.load(f)
            return history
        else:
            # è¿”å›ç©ºçš„èŠå¤©è®°å½•ç»“æ„
            return {
                "version": "1.0",
                "created": "",
                "updated": "",
                "messages": []
            }
    except Exception as e:
        print(f"Error loading chat history: {e}")
        return {"error": "Failed to load chat history"}

@app.post("/chat_history")
async def save_chat_history(request: Request):
    """ä¿å­˜èŠå¤©è®°å½•"""
    try:
        chat_data = await request.json()
        history_file = "chat_history/chat.json"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs("chat_history", exist_ok=True)
        
        # ä¿å­˜èŠå¤©è®°å½•
        with open(history_file, 'w', encoding='utf-8') as f:
            json.dump(chat_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Chat history saved with {len(chat_data.get('messages', []))} messages")
        return {"success": True, "message": "Chat history saved successfully"}
    except Exception as e:
        print(f"Error saving chat history: {e}")
        return {"error": "Failed to save chat history"}

@app.delete("/chat_history")
async def clear_chat_history():
    """æ¸…ç©ºèŠå¤©è®°å½•"""
    try:
        history_file = "chat_history/chat.json"
        if os.path.exists(history_file):
            os.remove(history_file)
            print("âœ… Chat history cleared")
        return {"success": True, "message": "Chat history cleared"}
    except Exception as e:
        print(f"Error clearing chat history: {e}")
        return {"error": "Failed to clear chat history"}

if __name__ == "__main__":
    uvicorn.run(app, host="localhost", port=8000)
    



